---
title: "USAJobs Data Rationalization Analysis"
author: "Enhanced Pipeline Analysis"
subtitle: "Analysis of unified dataset combining historical and current USAJobs data"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
execute:
  echo: true
  warning: false
---

```{python}
import duckdb
import pandas as pd
import numpy as np
import json
from collections import Counter
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# Connect to the unified dataset
conn = duckdb.connect("data/unified_pipeline_20250612_132347.duckdb")
```

## Dataset Overview

```{python}
# Basic dataset statistics
total_records = conn.execute("SELECT COUNT(*) FROM unified_jobs").fetchone()[0]
date_range = conn.execute("SELECT MIN(open_date), MAX(open_date) FROM unified_jobs").fetchone()
data_sources = conn.execute("SELECT data_sources, COUNT(*) FROM unified_jobs GROUP BY data_sources").fetchall()

print(f"üìä Total Records: {total_records:,}")
print(f"üìÖ Date Range: {date_range[0]} to {date_range[1]}")
print(f"üìà Average Confidence: {conn.execute('SELECT AVG(confidence_score) FROM unified_jobs').fetchone()[0]:.3f}")
print("\nüîÑ Data Sources:")
for source, count in data_sources:
    print(f"   {json.loads(source)[0] if source else 'Unknown'}: {count:,} records")
```

## Source Data Comparison

```{python}
# Find overlapping jobs (same control numbers in both datasets)
overlap_jobs = conn.execute("""
    WITH historical_controls AS (
        SELECT DISTINCT control_number 
        FROM unified_jobs 
        WHERE data_sources LIKE '%historical%'
    ),
    current_controls AS (
        SELECT DISTINCT control_number 
        FROM unified_jobs 
        WHERE data_sources LIKE '%current%'
    )
    SELECT h.control_number
    FROM historical_controls h
    INNER JOIN current_controls c ON h.control_number = c.control_number
    ORDER BY RANDOM()
    LIMIT 5
""").fetchall()

print("üîç SIDE-BY-SIDE COMPARISON: SAME JOBS IN BOTH DATASETS")
print("=" * 80)

if overlap_jobs:
    print(f"Found {len(overlap_jobs)} overlapping jobs to compare\n")
    
    for i, (control_num,) in enumerate(overlap_jobs, 1):
        print(f"JOB {i}: Control Number {control_num}")
        print("=" * 60)
        
        # Get historical version  
        hist_job = conn.execute("""
            SELECT control_number, position_title, agency_name, job_series, 
                   min_salary, max_salary, work_schedule, locations,
                   major_duties, qualification_summary, requirements,
                   job_summary, total_openings, promotion_potential,
                   security_clearance_required, telework_eligible, remote_indicator,
                   relocation_assistance, education, how_to_apply
            FROM unified_jobs 
            WHERE control_number = ? AND data_sources LIKE '%historical%'
        """, [control_num]).fetchone()
        
        # Get current version
        curr_job = conn.execute("""
            SELECT control_number, position_title, agency_name, job_series,
                   min_salary, max_salary, work_schedule, locations,
                   major_duties, qualification_summary, requirements,
                   job_summary, total_openings, promotion_potential,
                   security_clearance_required, telework_eligible, remote_indicator,
                   relocation_assistance, education, how_to_apply
            FROM unified_jobs
            WHERE control_number = ? AND data_sources LIKE '%current%'
        """, [control_num]).fetchone()
        
        # Field names for display - Core fields first, then additional fields
        field_names = ['Control Number', 'Position Title', 'Agency', 'Job Series', 
                      'Min Salary', 'Max Salary', 'Work Schedule', 'Locations',
                      'Major Duties', 'Qualification Summary', 'Requirements',
                      'Job Summary', 'Total Openings', 'Promotion Potential', 
                      'Security Clearance', 'Telework Eligible', 'Remote Indicator',
                      'Relocation Assistance', 'Education', 'How To Apply']
        
        print(f"{'Field':<20} {'Historical API':<60} {'Current API':<60}")
        print("-" * 140)
        
        for j, field_name in enumerate(field_names):
            hist_val = hist_job[j] if hist_job else "‚ùå Missing"
            curr_val = curr_job[j] if curr_job else "‚ùå Missing"
            
            # Format values for display
            def format_val(val):
                if val is None:
                    return "‚ùå None"
                elif str(val).strip() == "":
                    return "‚ö†Ô∏è Empty"
                elif isinstance(val, (int, float)) and field_name in ['Min Salary', 'Max Salary']:
                    return f"${val:,.0f}" if val else "‚ùå None"
                else:
                    val_str = str(val)
                    # For long text fields, show more content but still truncate
                    if field_name in ['Major Duties', 'Qualification Summary', 'Requirements']:
                        return val_str[:57] + "..." if len(val_str) > 60 else val_str
                    else:
                        return val_str[:57] + "..." if len(val_str) > 60 else val_str
            
            print(f"{field_name:<20} {format_val(hist_val):<60} {format_val(curr_val):<60}")
        
        print("\n")
else:
    print("‚ùå No overlapping jobs found between historical and current datasets")
```

```{python}
# Field coverage comparison between sources
field_coverage = conn.execute("""
    WITH source_stats AS (
        SELECT 
            CASE WHEN data_sources LIKE '%historical_api%' THEN 'Historical'
                 WHEN data_sources LIKE '%current_api%' THEN 'Current' 
                 ELSE 'Other' END as source_type,
            COUNT(*) as total_records,
            
            -- Core identification fields
            COUNT(CASE WHEN position_title IS NOT NULL AND position_title != '' THEN 1 END) as has_title,
            COUNT(CASE WHEN agency_name IS NOT NULL AND agency_name != '' THEN 1 END) as has_agency,
            COUNT(CASE WHEN job_series IS NOT NULL AND job_series != '' AND job_series != '0000' THEN 1 END) as has_series,
            
            -- Salary information
            COUNT(CASE WHEN min_salary IS NOT NULL AND max_salary IS NOT NULL THEN 1 END) as has_salary,
            
            -- Location data
            COUNT(CASE WHEN locations IS NOT NULL AND locations != '' THEN 1 END) as has_location,
            
            -- Rich content fields
            COUNT(CASE WHEN major_duties IS NOT NULL AND major_duties != '' THEN 1 END) as has_duties,
            COUNT(CASE WHEN qualification_summary IS NOT NULL AND qualification_summary != '' THEN 1 END) as has_qualifications,
            COUNT(CASE WHEN requirements IS NOT NULL AND requirements != '' THEN 1 END) as has_requirements,
            
            -- Workflow fields
            COUNT(CASE WHEN work_schedule IS NOT NULL AND work_schedule != '' THEN 1 END) as has_schedule
            
        FROM unified_jobs 
        WHERE data_sources NOT LIKE '%other%'
        GROUP BY source_type
    )
    SELECT * FROM source_stats WHERE source_type IN ('Historical', 'Current')
""").fetchall()

print("\nüìä FIELD COVERAGE BY SOURCE TYPE")
print("=" * 80)

if field_coverage:
    historical_stats = next((row for row in field_coverage if row[0] == 'Historical'), None)
    current_stats = next((row for row in field_coverage if row[0] == 'Current'), None)
    
    field_names = [
        ('Total Records', 1),
        ('Position Title', 2), ('Agency Name', 3), ('Job Series', 4),
        ('Salary Range', 5), ('Location', 6), 
        ('Major Duties', 7), ('Qualifications', 8), ('Requirements', 9),
        ('Work Schedule', 10)
    ]
    
    print(f"{'Field':<20} {'Historical':<15} {'Current':<15} {'Comparison'}")
    print("-" * 70)
    
    for field_name, idx in field_names:
        hist_val = historical_stats[idx] if historical_stats else 0
        curr_val = current_stats[idx] if current_stats else 0
        
        if idx == 1:  # Total records
            print(f"{field_name:<20} {hist_val:<15,} {curr_val:<15,} {'üìä Counts'}")
        else:
            hist_pct = (hist_val / historical_stats[1] * 100) if historical_stats and historical_stats[1] > 0 else 0
            curr_pct = (curr_val / current_stats[1] * 100) if current_stats and current_stats[1] > 0 else 0
            
            if hist_pct > curr_pct + 10:
                comparison = "üü¢ Hist Better"
            elif curr_pct > hist_pct + 10:
                comparison = "üîµ Curr Better"  
            elif abs(hist_pct - curr_pct) <= 10:
                comparison = "üü° Similar"
            else:
                comparison = "‚ö™ Mixed"
                
            print(f"{field_name:<20} {hist_pct:<15.1f}% {curr_pct:<15.1f}% {comparison}")

print(f"\nüîç KEY INSIGHTS:")
print(f"‚Ä¢ Historical API: Structured data with consistent core fields")
print(f"‚Ä¢ Current API: Rich content fields like duties and qualifications") 
print(f"‚Ä¢ Rationalization: Combines best of both sources")
```

## Field Rationalization Quality Analysis

### 1. Position Titles

```{python}
# Analyze position title rationalization
title_analysis = conn.execute("""
    SELECT 
        position_title,
        COUNT(*) as count,
        COUNT(DISTINCT agency_name) as unique_agencies,
        COUNT(DISTINCT job_series) as unique_series,
        AVG(confidence_score) as avg_confidence
    FROM unified_jobs 
    WHERE position_title IS NOT NULL 
    GROUP BY position_title 
    ORDER BY count DESC 
    LIMIT 15
""").fetchall()

title_df = pd.DataFrame(title_analysis, columns=['Position Title', 'Count', 'Unique Agencies', 'Unique Series', 'Avg Confidence'])
print("üè∑Ô∏è Most Common Position Titles:")
print(title_df.to_string(index=False))

# Title completeness
title_completeness = conn.execute("""
    SELECT 
        CASE WHEN position_title IS NULL OR position_title = '' THEN 'Missing' ELSE 'Present' END as status,
        COUNT(*) as count
    FROM unified_jobs 
    GROUP BY status
""").fetchall()

print(f"\nüìã Title Completeness:")
for status, count in title_completeness:
    pct = (count / total_records) * 100
    print(f"   {status}: {count:,} ({pct:.1f}%)")
```

### 2. Agency Name Rationalization

```{python}
# Analyze agency rationalization
agency_analysis = conn.execute("""
    SELECT 
        agency_name,
        department_name,
        COUNT(*) as job_count,
        COUNT(DISTINCT job_series) as unique_series,
        MIN(open_date) as earliest_job,
        MAX(open_date) as latest_job
    FROM unified_jobs 
    WHERE agency_name IS NOT NULL 
    GROUP BY agency_name, department_name
    ORDER BY job_count DESC 
    LIMIT 10
""").fetchall()

agency_df = pd.DataFrame(agency_analysis, columns=['Agency', 'Department', 'Job Count', 'Unique Series', 'Earliest', 'Latest'])
print("üè¢ Top Hiring Agencies:")
print(agency_df.to_string(index=False))

# Agency-Department consistency
consistency_check = conn.execute("""
    SELECT 
        agency_name,
        COUNT(DISTINCT department_name) as dept_variations
    FROM unified_jobs 
    WHERE agency_name IS NOT NULL AND department_name IS NOT NULL
    GROUP BY agency_name 
    HAVING COUNT(DISTINCT department_name) > 1
    ORDER BY dept_variations DESC
    LIMIT 5
""").fetchall()

if consistency_check:
    print(f"\n‚ö†Ô∏è Agencies with Multiple Departments (potential inconsistencies):")
    for agency, var_count in consistency_check:
        print(f"   {agency}: {var_count} departments")
```

### 3. Job Series (Occupational Codes) Analysis

```{python}
# Job series rationalization analysis
series_analysis = conn.execute("""
    SELECT 
        job_series,
        COUNT(*) as count,
        COUNT(DISTINCT agency_name) as unique_agencies
    FROM unified_jobs 
    WHERE job_series IS NOT NULL AND job_series != '' AND job_series != '0000'
    GROUP BY job_series 
    ORDER BY count DESC 
    LIMIT 15
""").fetchall()

print("üî¢ Most Common Job Series:")
for series, count, agencies in series_analysis:
    print(f"   {series}: {count:,} jobs, {agencies} agencies")

# Job series completeness
series_completeness = conn.execute("""
    SELECT 
        CASE 
            WHEN job_series IS NULL OR job_series = '' OR job_series = '0000' THEN 'Missing/Invalid' 
            ELSE 'Valid' 
        END as status,
        COUNT(*) as count
    FROM unified_jobs 
    GROUP BY status
""").fetchall()

print(f"üìä Job Series Completeness:")
for status, count in series_completeness:
    pct = (count / total_records) * 100
    print(f"   {status}: {count:,} ({pct:.1f}%)")
```

### 4. Major Duties Field Analysis

```{python}
# Analyze major duties field (key for understanding job content)
duties_analysis = conn.execute("""
    SELECT 
        CASE 
            WHEN major_duties IS NULL OR major_duties = '' THEN 'Missing'
            WHEN LENGTH(major_duties) < 100 THEN 'Brief (< 100 chars)'
            WHEN LENGTH(major_duties) < 500 THEN 'Moderate (100-500 chars)'
            WHEN LENGTH(major_duties) < 1000 THEN 'Detailed (500-1000 chars)'
            ELSE 'Very Detailed (1000+ chars)'
        END as duties_category,
        COUNT(*) as count,
        AVG(confidence_score) as avg_confidence
    FROM unified_jobs 
    GROUP BY duties_category
    ORDER BY count DESC
""").fetchall()

print("üìù Major Duties Field Analysis:")
for category, count, confidence in duties_analysis:
    pct = (count / total_records) * 100
    conf_str = f"(confidence: {confidence:.2f})" if confidence else "(confidence: N/A)"
    print(f"   {category}: {count:,} ({pct:.1f}%) {conf_str}")

# Sample duties by length category
sample_duties = conn.execute("""
    SELECT 
        position_title,
        agency_name,
        LENGTH(major_duties) as duties_length,
        LEFT(major_duties, 200) as duties_sample
    FROM unified_jobs 
    WHERE major_duties IS NOT NULL AND major_duties != ''
    ORDER BY RANDOM()
    LIMIT 3
""").fetchall()

print(f"\nüìã Sample Major Duties Content:")
for title, agency, length, sample in sample_duties:
    print(f"   {title} ({agency})")
    print(f"   Length: {length} characters")
    print(f"   Sample: {sample}...")
    print()
```

## Data Quality Metrics

```{python}
# Comprehensive data quality analysis
quality_metrics = conn.execute("""
    SELECT 
        'Position Title' as field,
        COUNT(*) as total,
        COUNT(position_title) as non_null,
        ROUND(COUNT(position_title) * 100.0 / COUNT(*), 1) as completeness_pct
    FROM unified_jobs
    
    UNION ALL
    
    SELECT 
        'Agency Name' as field,
        COUNT(*) as total,
        COUNT(agency_name) as non_null,
        ROUND(COUNT(agency_name) * 100.0 / COUNT(*), 1) as completeness_pct
    FROM unified_jobs
    
    UNION ALL
    
    SELECT 
        'Job Series' as field,
        COUNT(*) as total,
        COUNT(CASE WHEN job_series IS NOT NULL AND job_series != '' AND job_series != '0000' THEN 1 END) as non_null,
        ROUND(COUNT(CASE WHEN job_series IS NOT NULL AND job_series != '' AND job_series != '0000' THEN 1 END) * 100.0 / COUNT(*), 1) as completeness_pct
    FROM unified_jobs
    
    UNION ALL
    
    SELECT 
        'Major Duties' as field,
        COUNT(*) as total,
        COUNT(major_duties) as non_null,
        ROUND(COUNT(major_duties) * 100.0 / COUNT(*), 1) as completeness_pct
    FROM unified_jobs
    
    UNION ALL
    
    SELECT 
        'Salary Range' as field,
        COUNT(*) as total,
        COUNT(CASE WHEN min_salary IS NOT NULL AND max_salary IS NOT NULL THEN 1 END) as non_null,
        ROUND(COUNT(CASE WHEN min_salary IS NOT NULL AND max_salary IS NOT NULL THEN 1 END) * 100.0 / COUNT(*), 1) as completeness_pct
    FROM unified_jobs
    
    UNION ALL
    
    SELECT 
        'Locations' as field,
        COUNT(*) as total,
        COUNT(locations) as non_null,
        ROUND(COUNT(locations) * 100.0 / COUNT(*), 1) as completeness_pct
    FROM unified_jobs
""").fetchall()

quality_df = pd.DataFrame(quality_metrics, columns=['Field', 'Total Records', 'Complete Records', 'Completeness %'])
print("üìä Field Completeness Analysis:")
print(quality_df.to_string(index=False))
```

## Rationalization Effectiveness

```{python}
# Analyze how well the rationalization process worked
confidence_distribution = conn.execute("""
    SELECT 
        CASE 
            WHEN confidence_score >= 0.95 THEN 'Excellent (0.95+)'
            WHEN confidence_score >= 0.90 THEN 'Good (0.90-0.94)'
            WHEN confidence_score >= 0.85 THEN 'Fair (0.85-0.89)'
            ELSE 'Poor (<0.85)'
        END as confidence_level,
        COUNT(*) as count
    FROM unified_jobs 
    GROUP BY confidence_level
    ORDER BY MIN(confidence_score) DESC
""").fetchall()

print("üéØ Rationalization Confidence Distribution:")
for level, count in confidence_distribution:
    pct = (count / total_records) * 100
    print(f"   {level}: {count:,} ({pct:.1f}%)")

# Source combination analysis
source_combinations = conn.execute("""
    SELECT 
        data_sources,
        COUNT(*) as count,
        AVG(confidence_score) as avg_confidence
    FROM unified_jobs 
    GROUP BY data_sources
    ORDER BY count DESC
""").fetchall()

print(f"\nüì° Data Source Combinations:")
for sources, count, confidence in source_combinations:
    source_list = json.loads(sources) if sources else ['Unknown']
    pct = (count / total_records) * 100
    print(f"   {', '.join(source_list)}: {count:,} ({pct:.1f}%) - confidence: {confidence:.3f}")
```

## Sample High-Quality Records

```{python}
# Show examples of well-rationalized records
high_quality_samples = conn.execute("""
    SELECT 
        control_number,
        position_title,
        agency_name,
        job_series,
        CASE WHEN major_duties IS NOT NULL THEN 'Yes' ELSE 'No' END as has_duties,
        CASE WHEN min_salary IS NOT NULL AND max_salary IS NOT NULL THEN 'Yes' ELSE 'No' END as has_salary,
        confidence_score
    FROM unified_jobs 
    WHERE confidence_score >= 0.90 
        AND position_title IS NOT NULL 
        AND agency_name IS NOT NULL 
        AND job_series IS NOT NULL 
        AND job_series != '' 
        AND job_series != '0000'
    ORDER BY confidence_score DESC, RANDOM()
    LIMIT 10
""").fetchall()

print("‚≠ê Sample High-Quality Rationalized Records:")
print()
for control, title, agency, series, duties, salary, confidence in high_quality_samples:
    print(f"Control: {control}")
    print(f"Title: {title}")
    print(f"Agency: {agency}")
    print(f"Series: {series}")
    print(f"Has Duties: {duties} | Has Salary: {salary}")
    print(f"Confidence: {confidence:.3f}")
    print("-" * 50)
```

## Geographic Distribution

```{python}
# Analyze location data
location_analysis = conn.execute("""
    SELECT 
        CASE 
            WHEN locations LIKE '%Washington%' THEN 'Washington DC Area'
            WHEN locations LIKE '%California%' THEN 'California'
            WHEN locations LIKE '%Texas%' THEN 'Texas'
            WHEN locations LIKE '%New York%' THEN 'New York'
            WHEN locations LIKE '%Virginia%' THEN 'Virginia'
            WHEN locations LIKE '%Maryland%' THEN 'Maryland'
            WHEN locations LIKE '%Remote%' OR locations LIKE '%remote%' THEN 'Remote'
            WHEN locations LIKE '%Multiple%' OR locations LIKE '%multiple%' THEN 'Multiple Locations'
            ELSE 'Other States'
        END as location_category,
        COUNT(*) as count
    FROM unified_jobs 
    WHERE locations IS NOT NULL
    GROUP BY location_category
    ORDER BY count DESC
    LIMIT 10
""").fetchall()

print("üó∫Ô∏è Geographic Distribution of Jobs:")
for location, count in location_analysis:
    pct = (count / total_records) * 100
    print(f"   {location}: {count:,} ({pct:.1f}%)")
```

## Summary Assessment

```{python}
# Overall assessment of rationalization quality
total_high_quality = conn.execute("SELECT COUNT(*) FROM unified_jobs WHERE confidence_score >= 0.90").fetchone()[0]
total_complete_core = conn.execute("""
    SELECT COUNT(*) FROM unified_jobs 
    WHERE position_title IS NOT NULL 
        AND agency_name IS NOT NULL 
        AND job_series IS NOT NULL 
        AND job_series != '' 
        AND job_series != '0000'
""").fetchone()[0]

print("üéØ RATIONALIZATION QUALITY SUMMARY")
print("=" * 50)
print(f"‚úÖ High Confidence Records (‚â•0.90): {total_high_quality:,} ({(total_high_quality/total_records)*100:.1f}%)")
print(f"‚úÖ Complete Core Fields: {total_complete_core:,} ({(total_complete_core/total_records)*100:.1f}%)")
print(f"‚úÖ Average Confidence Score: {conn.execute('SELECT AVG(confidence_score) FROM unified_jobs').fetchone()[0]:.3f}")

# Close connection
conn.close()

print(f"\nüîç Key Findings:")
print(f"‚Ä¢ Position titles are well-preserved with high completeness")
print(f"‚Ä¢ Agency names show good consistency across departments") 
print(f"‚Ä¢ Job series codes are properly rationalized for most records")
print(f"‚Ä¢ Major duties field needs enhancement from scraping for completeness")
print(f"‚Ä¢ Overall data quality is good with strong confidence scores")
```