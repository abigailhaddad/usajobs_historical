#!/usr/bin/env python3
"""
Enhanced USAJobs Data Pipeline

A complete pipeline that:
1. Fetches current job postings from USAJobs API
2. Scrapes detailed content from job posting pages  
3. Integrates with historical job data
4. Rationalizes fields between data sources
5. Generates comprehensive analysis report

Usage:
    python run_pipeline.py [options]

Options:
    --historical-jobs N     Number of recent historical jobs to process (default: all since start-date)
    --start-date YYYY-MM-DD Start date for job filtering (default: 2025-01-01)
    --output-name NAME     Custom name for output files (default: auto-generated)
    --no-report            Skip HTML report generation (reports are generated by default)
"""

import argparse
import subprocess
import sys
import os
import duckdb
import json
import requests
import time
from pathlib import Path
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm


def setup_directories():
    """Ensure required directories exist"""
    base_dir = Path(__file__).parent
    directories = ['data', 'logs']
    
    for dir_name in directories:
        dir_path = base_dir / dir_name
        dir_path.mkdir(exist_ok=True)
    
    return base_dir
def fetch_jobs_for_date_range(date_str, output_name, base_dir):
    """Fetch jobs for a specific date and save to individual database"""
    try:
        print(f"üîé Fetching jobs for {date_str}")
        day_jobs = fetch_all_jobs_for_date(date_str)
        
        if day_jobs:
            # Create individual database for this date
            db_path = base_dir / "data" / f"temp_jobs_{date_str}_{output_name}.duckdb"
            create_historical_database_for_jobs(day_jobs, db_path)
            print(f"  ‚úÖ {len(day_jobs)} jobs saved to {db_path.name}")
            return db_path, len(day_jobs)
        else:
            print(f"  üì≠ No jobs found for {date_str}")
            return None, 0
            
    except Exception as e:
        print(f"  ‚ö†Ô∏è Error for {date_str}: {e}")
        return None, 0

def fetch_recent_historical_jobs_parallel(num_jobs=None, start_date_str='2025-01-01', output_name="", max_workers=4):
    """Fetch recent historical jobs using parallel processing by date"""
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
    print(f"üìä Fetching {'ALL' if not num_jobs else num_jobs} historical jobs since {start_date_str} (parallel)...")

    end_date = datetime.now()
    days_to_fetch = (end_date - start_date).days
    base_dir = Path(__file__).parent

    # Generate list of dates to fetch
    dates_to_fetch = []
    for days_back in range(days_to_fetch):
        date = end_date - timedelta(days=days_back)
        dates_to_fetch.append(date.strftime('%Y-%m-%d'))

    print(f"üóìÔ∏è Processing {len(dates_to_fetch)} dates with {max_workers} workers")

    temp_dbs = []
    total_jobs = 0
    
    # Process dates in parallel
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_date = {
            executor.submit(fetch_jobs_for_date_range, date_str, output_name, base_dir): date_str 
            for date_str in dates_to_fetch
        }
        
        # Collect results as they complete with progress bar
        with tqdm(total=len(dates_to_fetch), desc="üìÖ Fetching by date", unit="date") as pbar:
            for future in as_completed(future_to_date):
                date_str = future_to_date[future]
                try:
                    db_path, job_count = future.result()
                    if db_path:
                        temp_dbs.append(db_path)
                        total_jobs += job_count
                        pbar.set_postfix({"jobs": total_jobs, "files": len(temp_dbs)})
                        
                        # Stop early if we have enough jobs
                        if num_jobs is not None and total_jobs >= num_jobs:
                            print(f"üéØ Reached target of {num_jobs} jobs, stopping early")
                            # Cancel remaining futures
                            for f in future_to_date:
                                if not f.done():
                                    f.cancel()
                            break
                            
                except Exception as e:
                    print(f"  ‚ùå Failed to process {date_str}: {e}")
                finally:
                    pbar.update(1)

    print(f"üì¶ Collected {total_jobs} total jobs across {len(temp_dbs)} date files")
    return temp_dbs, total_jobs

def fetch_recent_historical_jobs_sequential(num_jobs=None, start_date_str='2025-01-01'):
    """Fetch recent historical jobs using sequential processing (no temp files)"""
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
    print(f"üìä Fetching {'ALL' if not num_jobs else num_jobs} historical jobs since {start_date_str} (sequential)...")

    end_date = datetime.now()
    days_to_fetch = (end_date - start_date).days

    all_jobs = []
    seen_control_numbers = set()

    # Generate dates to fetch
    dates_to_fetch = []
    for days_back in range(days_to_fetch):
        date = end_date - timedelta(days=days_back)
        dates_to_fetch.append(date.strftime('%Y-%m-%d'))

    # Process dates sequentially with progress bar
    with tqdm(dates_to_fetch, desc="üìÖ Fetching by date", unit="date") as pbar:
        for date_str in pbar:
            try:
                day_jobs = fetch_all_jobs_for_date(date_str)

                # Deduplicate by control number
                new_jobs = [j for j in day_jobs if j.get("usajobsControlNumber") not in seen_control_numbers]
                all_jobs.extend(new_jobs)
                seen_control_numbers.update(j.get("usajobsControlNumber") for j in new_jobs)

                pbar.set_postfix({"jobs": len(all_jobs)})

                # Stop early if we have enough jobs
                if num_jobs is not None and len(all_jobs) >= num_jobs:
                    print(f"üéØ Reached target of {num_jobs} jobs, stopping early")
                    break

            except Exception as e:
                print(f"  ‚ö†Ô∏è Error for {date_str}: {e}")
                continue

            time.sleep(0.1)

    print(f"üì¶ Collected {len(all_jobs)} total jobs")
    return all_jobs

def fetch_all_jobs_for_date(date_str):
    """Fetch all jobs for a specific date using proper continuation pagination"""
    base_url = "https://data.usajobs.gov"
    endpoint = "/api/historicjoa"
    params = {
        "StartPositionOpenDate": date_str,
        "EndPositionOpenDate": date_str
    }

    all_jobs = []
    next_url = endpoint  # Start with base endpoint

    while True:
        if next_url == endpoint:
            response = requests.get(f"{base_url}{endpoint}", params=params)
        else:
            response = requests.get(f"{base_url}{next_url}")

        if response.status_code != 200:
            raise Exception(f"Status {response.status_code}: {response.text[:200]}")

        data = response.json()
        page_jobs = data.get("data", [])
        all_jobs.extend(page_jobs)

        paging = data.get("paging", {})
        next_url = paging.get("next")

        if not next_url:
            break

        time.sleep(0.2)

    return all_jobs


def create_historical_database_for_jobs(jobs, db_path):
    """Create historical jobs database for a specific set of jobs"""
    conn = duckdb.connect(str(db_path))
    
    # Create historical jobs table
    conn.execute("""
        CREATE TABLE IF NOT EXISTS historical_jobs (
            control_number BIGINT PRIMARY KEY,
            announcement_number VARCHAR,
            hiring_agency_name VARCHAR,
            hiring_department_name VARCHAR,
            hiring_subelement_name VARCHAR,
            position_title VARCHAR,
            minimum_grade VARCHAR,
            maximum_grade VARCHAR,
            minimum_salary DECIMAL,
            maximum_salary DECIMAL,
            position_open_date DATE,
            position_close_date DATE,
            locations VARCHAR,
            work_schedule VARCHAR,
            travel_requirement VARCHAR,
            job_series VARCHAR,
            raw_data JSON,
            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # Create scraped jobs table
    conn.execute("""
        CREATE TABLE IF NOT EXISTS scraped_jobs (
            control_number VARCHAR PRIMARY KEY,
            scraped_date TIMESTAMP,
            scraped_content JSON,
            scraping_success BOOLEAN,
            error_message VARCHAR
        )
    """)
    
    # Insert historical jobs
    for job in jobs:
        try:
            control_number = job.get("usajobsControlNumber")
            if not control_number:
                continue
                
            conn.execute("""
                INSERT OR REPLACE INTO historical_jobs (
                    control_number, announcement_number, hiring_agency_name,
                    hiring_department_name, hiring_subelement_name, position_title,
                    minimum_grade, maximum_grade, minimum_salary, maximum_salary,
                    position_open_date, position_close_date, locations,
                    work_schedule, travel_requirement, job_series, raw_data
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, [
                control_number,
                job.get("announcementNumber"),
                job.get("hiringAgencyName"),
                job.get("hiringDepartmentName"), 
                job.get("hiringSubelementName"),
                job.get("positionTitle"),
                job.get("minimumGrade"),
                job.get("maximumGrade"),
                job.get("minimumSalary"),
                job.get("maximumSalary"),
                job.get("positionOpenDate"),
                job.get("positionCloseDate"),
                " | ".join([f"{loc.get('positionLocationCity', '')}, {loc.get('positionLocationState', '')}" 
                           for loc in job.get("PositionLocations", [])]),
                job.get("workSchedule"),
                job.get("travelRequirement"),
                ", ".join([c.get("series", "") for c in job.get("JobCategories", [])]),
                json.dumps(job)
            ])
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error inserting job {control_number}: {e}")
            continue
    
    conn.close()

def merge_historical_databases(temp_dbs, output_name):
    """Merge multiple temporary databases into final historical database"""
    print(f"üîó Merging {len(temp_dbs)} temporary databases...")
    
    base_dir = Path(__file__).parent
    final_db_path = base_dir / "data" / f"historical_jobs_{output_name}.duckdb"
    
    # Create final database
    conn = duckdb.connect(str(final_db_path))
    
    # Drop existing tables
    conn.execute("DROP TABLE IF EXISTS historical_jobs")
    conn.execute("DROP TABLE IF EXISTS scraped_jobs")
    
    # Create tables with same structure
    conn.execute("""
        CREATE TABLE historical_jobs (
            control_number BIGINT PRIMARY KEY,
            announcement_number VARCHAR,
            hiring_agency_name VARCHAR,
            hiring_department_name VARCHAR,
            hiring_subelement_name VARCHAR,
            position_title VARCHAR,
            minimum_grade VARCHAR,
            maximum_grade VARCHAR,
            minimum_salary DECIMAL,
            maximum_salary DECIMAL,
            position_open_date DATE,
            position_close_date DATE,
            locations VARCHAR,
            work_schedule VARCHAR,
            travel_requirement VARCHAR,
            job_series VARCHAR,
            raw_data JSON,
            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    conn.execute("""
        CREATE TABLE scraped_jobs (
            control_number VARCHAR PRIMARY KEY,
            scraped_date TIMESTAMP,
            scraped_content JSON,
            scraping_success BOOLEAN,
            error_message VARCHAR
        )
    """)
    
    total_jobs = 0
    
    # Merge each temporary database with progress bar
    with tqdm(temp_dbs, desc="üîó Merging databases", unit="file") as pbar:
        for temp_db in pbar:
            try:
                # Attach temporary database (escape single quotes in path)
                safe_path = str(temp_db).replace("'", "''")
                conn.execute(f"ATTACH '{safe_path}' AS temp_db")
                
                # Insert data with deduplication using ON CONFLICT
                result = conn.execute("""
                    INSERT INTO historical_jobs 
                    SELECT * FROM temp_db.historical_jobs
                    ON CONFLICT (control_number) DO NOTHING
                """)
                
                # Get count of inserted rows
                count = conn.execute("SELECT changes()").fetchone()[0]
                total_jobs += count
                pbar.set_postfix({"total_jobs": total_jobs})
                
                # Detach temporary database
                conn.execute("DETACH temp_db")
                
                # Clean up temporary file
                temp_db.unlink()
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è Error merging {temp_db}: {e}")
    
    conn.close()
    print(f"  ‚úÖ Created final database with {total_jobs} unique jobs")
    return final_db_path

def create_historical_database(jobs, output_name):
    """Create historical jobs database (legacy function for backwards compatibility)"""
    print("üíæ Creating historical jobs database...")
    
    base_dir = Path(__file__).parent
    db_path = base_dir / "data" / f"historical_jobs_{output_name}.duckdb"
    
    create_historical_database_for_jobs(jobs, db_path)
    print(f"  ‚úÖ Created database with {len(jobs)} jobs")
    return db_path


def fetch_current_jobs():
    """Fetch current jobs using the API script"""
    print("üåê Fetching ALL current jobs...")
    
    current_script = Path(__file__).parent / "scripts" / "fetch_current_jobs.py"
    
    # Remove days-posted parameter to get ALL current jobs
    cmd = [sys.executable, str(current_script)]
    result = subprocess.run(cmd, cwd=current_script.parent)
    
    return result.returncode == 0


def scrape_single_job(control_number, db_path):
    """Scrape a single job posting"""
    try:
        # Import scraping function
        sys.path.append(str(Path(__file__).parent / "scripts"))
        from scrape_enhanced_job_posting import scrape_enhanced_job_posting
        
        conn = duckdb.connect(str(db_path))
        
        # Check if already scraped
        existing = conn.execute("SELECT control_number FROM scraped_jobs WHERE control_number = ?", [str(control_number)]).fetchone()
        if existing:
            conn.close()
            return True, f"Already scraped: {control_number}"
        
        result = scrape_enhanced_job_posting(str(control_number))
        
        conn.execute("""
            INSERT OR REPLACE INTO scraped_jobs 
            (control_number, scraped_date, scraped_content, scraping_success, error_message)
            VALUES (?, ?, ?, ?, ?)
        """, [
            str(control_number),
            datetime.now().isoformat(),
            json.dumps(result) if result else None,
            result.get('status') == 'success' if result else False,
            result.get('error') if result and result.get('status') == 'error' else None
        ])
        
        conn.close()
        
        if result and result.get('status') == 'success':
            return True, f"Successfully scraped: {control_number}"
        else:
            return False, f"Failed to scrape: {control_number}"
            
    except Exception as e:
        return False, f"Error scraping {control_number}: {e}"

def scrape_jobs_parallel(db_path, num_to_scrape=None, max_workers=8):
    """Scrape job postings for enhanced content using parallel processing"""
    print(f"üï∑Ô∏è Scraping job postings (parallel with {max_workers} workers)...")
    
    conn = duckdb.connect(str(db_path))
    
    # Get control numbers to scrape
    if num_to_scrape:
        control_numbers = conn.execute(f"SELECT control_number FROM historical_jobs LIMIT {num_to_scrape}").fetchall()
    else:
        control_numbers = conn.execute("SELECT control_number FROM historical_jobs").fetchall()
    
    conn.close()
    
    print(f"  üìÑ Scraping {len(control_numbers)} jobs...")
    
    success_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all scraping tasks
        future_to_control_number = {
            executor.submit(scrape_single_job, control_number[0], db_path): control_number[0] 
            for control_number in control_numbers
        }
        
        # Process completed tasks with progress bar
        with tqdm(total=len(control_numbers), desc="üï∑Ô∏è Scraping jobs", unit="job") as pbar:
            for future in as_completed(future_to_control_number):
                control_number = future_to_control_number[future]
                try:
                    success, message = future.result()
                    if success:
                        success_count += 1
                    pbar.set_postfix({"success": success_count, "rate": f"{success_count/pbar.n*100:.1f}%" if pbar.n > 0 else "0%"})
                        
                except Exception as e:
                    print(f"    ‚ùå Error processing {control_number}: {e}")
                finally:
                    pbar.update(1)
    
    print(f"  ‚úÖ Successfully scraped {success_count}/{len(control_numbers)} jobs")

def scrape_jobs(db_path, num_to_scrape=None):
    """Scrape job postings for enhanced content (legacy function)"""
    scrape_jobs_parallel(db_path, num_to_scrape)


def run_field_rationalization(historical_db, output_name):
    """Run field rationalization to create unified dataset"""
    print("üîÑ Running field rationalization...")
    
    base_dir = Path(__file__).parent
    data_dir = base_dir / "data"
    
    # Find current jobs file
    current_files = list(data_dir.glob("current_jobs_*.json"))
    if not current_files:
        print("  ‚ùå No current jobs files found")
        return None
    
    latest_current = max(current_files, key=os.path.getctime)
    
    rationalization_script = base_dir / "scripts" / "field_rationalization.py"
    output_file = data_dir / f"unified_{output_name}.duckdb"
    
    cmd = [
        sys.executable, str(rationalization_script),
        "--historical-db", str(historical_db),
        "--current-json", str(latest_current),
        "--output", str(output_file),
        "--output-format", "duckdb"
    ]
    
    result = subprocess.run(cmd, cwd=rationalization_script.parent, capture_output=True, text=True)
    
    if result.returncode == 0:
        print(f"  ‚úÖ Created unified dataset: {output_file.name}")
        return output_file
    else:
        print(f"  ‚ùå Rationalization failed:")
        print(f"    {result.stderr}")
        return None


def render_analysis_report(unified_db_path):
    """Update and render the analysis report"""
    print("üìù Generating analysis report...")
    
    base_dir = Path(__file__).parent
    qmd_file = base_dir / "rationalization_analysis.qmd"
    
    if not qmd_file.exists():
        print(f"  ‚ùå QMD file not found at {qmd_file}")
        return False
    
    # Update QMD to use the new database
    with open(qmd_file, 'r') as f:
        content = f.read()
    
    import re
    db_name = unified_db_path.name
    new_line = f'conn = duckdb.connect("data/{db_name}")'
    pattern = r'conn\s*=\s*duckdb\.connect\([\'"].*?[\'"]\)'
    content = re.sub(pattern, new_line, content)
    
    with open(qmd_file, 'w') as f:
        f.write(content)
    
    print(f"  ‚úÖ Updated QMD to use: {db_name}")
    
    # Render to HTML
    print("üé® Rendering HTML report...")
    cmd = ["quarto", "render", str(qmd_file)]
    result = subprocess.run(cmd, cwd=base_dir, capture_output=True, text=True)
    
    if result.returncode == 0:
        print(f"  ‚úÖ Report rendered successfully")
        return True
    else:
        print(f"  ‚ùå Render failed:")
        print(f"    {result.stderr}")
        return False


def main():
    parser = argparse.ArgumentParser(description='Enhanced USAJobs Data Pipeline')
    parser.add_argument('--historical-jobs', type=int, default=None, 
                       help='Number of recent historical jobs to process (default: all since start-date)')
    parser.add_argument('--start-date', type=str, default='2025-01-01',
                       help='Start date for job filtering (YYYY-MM-DD, default: 2025-01-01)')
    parser.add_argument('--output-name', 
                       help='Custom name for output files (default: auto-generated)')
    parser.add_argument('--no-report', action='store_true',
                       help='Skip HTML report generation (reports are generated by default)')
    parser.add_argument('--skip-fetch', action='store_true',
                       help='Skip historical job fetching (use existing database)')
    parser.add_argument('--skip-current', action='store_true',
                       help='Skip current job fetching (use existing file)')
    parser.add_argument('--skip-scraping', action='store_true',
                       help='Skip job scraping step')
    parser.add_argument('--scraping-only', action='store_true',
                       help='Only run scraping step (requires existing historical database)')
    
    args = parser.parse_args()
    
    # Setup
    base_dir = setup_directories()
    
    # Generate output name if not provided
    if not args.output_name:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        args.output_name = f"pipeline_{timestamp}"
    
    print("üöÄ ENHANCED USAJOBS DATA PIPELINE")
    print("=" * 50)
    print(f"üìä Historical jobs: {args.historical_jobs or 'ALL'} since {args.start_date}")
    print(f"üï∑Ô∏è Scraping: All historical jobs")
    print(f"üìÖ Current jobs: ALL available")
    print(f"üìÅ Output name: {args.output_name}")
    print("=" * 50)
    
    try:
        # Handle scraping-only mode
        if args.scraping_only:
            print("üï∑Ô∏è SCRAPING-ONLY MODE")
            historical_db = base_dir / "data" / f"historical_jobs_{args.output_name}.duckdb"
            if not historical_db.exists():
                print(f"‚ùå Historical database not found: {historical_db}")
                return 1
            
            print(f"üìä Using existing database: {historical_db.name}")
            scrape_jobs_parallel(historical_db, None)
            print("‚úÖ Scraping complete!")
            return 0
        
        # Step 1: Fetch historical jobs (using simple sequential approach for now)
        if not args.skip_fetch:
            print("üìä Using sequential fetching to avoid merge issues...")
            # Use the original non-parallel function
            historical_jobs = fetch_recent_historical_jobs_sequential(args.historical_jobs, args.start_date)
            if not historical_jobs:
                print("‚ùå No historical jobs found")
                return 1
            
            # Step 2: Create historical database directly
            historical_db = create_historical_database(historical_jobs, args.output_name)
        else:
            print("‚è≠Ô∏è Skipping historical job fetching")
            historical_db = base_dir / "data" / f"historical_jobs_{args.output_name}.duckdb"
            if not historical_db.exists():
                print(f"‚ùå Historical database not found: {historical_db}")
                return 1
        
        # Step 3: Scrape jobs in parallel (unless skipped)
        if not args.skip_scraping:
            scrape_jobs_parallel(historical_db, None)
        else:
            print("‚è≠Ô∏è Skipping job scraping")
        
        # Step 4: Fetch current jobs (unless skipped)
        if not args.skip_current:
            if not fetch_current_jobs():
                print("‚ùå Current jobs fetch failed")
                return 1
        else:
            print("‚è≠Ô∏è Skipping current job fetching")
        
        # Step 5: Run field rationalization
        unified_db = run_field_rationalization(historical_db, args.output_name)
        if not unified_db:
            print("‚ùå Field rationalization failed")
            return 1
        
        # Step 6: Render report (default, unless skipped)
        if not args.no_report:
            render_analysis_report(unified_db)
        
        print(f"\n‚úÖ PIPELINE COMPLETE!")
        print(f"üìä Unified dataset: {unified_db.name}")
        print(f"üìÅ Location: {unified_db}")
        
        if not args.no_report:
            print(f"üìÑ HTML report: rationalization_analysis.html")
        else:
            print(f"üí° To generate report: python run_pipeline.py --historical-jobs {args.historical_jobs or 'ALL'}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Pipeline interrupted by user")
        return 1
    except Exception as e:
        print(f"\n‚ùå Pipeline failed: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())