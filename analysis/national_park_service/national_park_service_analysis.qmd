---
title: "National Park Service Job Postings Analysis"
author: "Abigail Haddad"
date: today
format:
  html:
    output-file: index.html
    theme: cosmo
    toc: true
    toc-depth: 2
    toc-location: right
    code-fold: true
    code-tools: true
    fig-width: 12
    fig-height: 8
    self-contained: true
execute:
  echo: true
  warning: false
jupyter: python3
---

This analysis examines federal job posting trends for the National Park Service from 2018 through June 2025.

**Resources:** [GitHub Repository](https://github.com/abigailhaddad/usajobs_historical) | [Field Documentation](https://abigailhaddad.github.io/usajobs_historical/) | [USAJobs API](https://developer.usajobs.gov/)

## Data Loading and Preparation

```{python}
import pandas as pd
import numpy as np
from datetime import datetime
import json

from great_tables import GT, md

def create_standard_gt_table(data, title, subtitle="", align_left_cols=None, align_center_cols=None, col_widths=None, include_source=True):
    """Create a standardized Great Tables table with common formatting"""
    # Add USAJobs data attribution to subtitle if not already included
    if include_source and subtitle and "USAJobs" not in subtitle:
        subtitle = f"{subtitle} | USAJobs Historical Data"
    elif include_source and not subtitle:
        subtitle = "USAJobs Historical Data"
    
    # Start with basic table
    gt_table = GT(data.reset_index(drop=True))
    gt_table = gt_table.tab_header(title=title, subtitle=subtitle)
    gt_table = gt_table.tab_source_note(md("*Source: github.com/abigailhaddad/usajobs_historical*"))
    
    # Apply alignments
    if align_left_cols:
        gt_table = gt_table.cols_align(align="left", columns=align_left_cols)
    if align_center_cols:
        gt_table = gt_table.cols_align(align="center", columns=align_center_cols)
    
    # Apply widths
    if col_widths:
        gt_table = gt_table.cols_width(col_widths)
    
    # Apply options without width constraints
    gt_table = gt_table.tab_options(
        quarto_disable_processing=True
    )
    
    return gt_table

def get_current_datetime():
    """Get current date and time info for consistent usage"""
    current_date = datetime.now()
    return {
        'date': current_date,
        'year': current_date.year,
        'month': current_date.month,
        'formatted': current_date.strftime('%Y-%m-%d %H:%M:%S')
    }

def extract_series(job_categories):
    """Extract occupational series from JobCategories JSON field"""
    try:
        if pd.isna(job_categories):
            return 'Unknown'
        categories = json.loads(job_categories)
        if categories and len(categories) > 0 and 'series' in categories[0]:
            return categories[0]['series']
        return 'Unknown'
    except:
        return 'Unknown'

def categorize_appointment(appt_type):
    """Categorize appointment types into Permanent, Term/Temporary, or Other"""
    if appt_type == 'Permanent':
        return 'Permanent'
    elif appt_type in ['Term', 'Temporary', 'Seasonal', 'Summer', 'Intermittent', 'Internships']:
        return 'Term/Temporary'
    else:
        return 'Other'

def load_nps_data():
    """Load and prepare National Park Service job data"""
    # Load all years from 2018 onwards
    years = range(2018, 2026)
    all_data = []
    year_counts = []
    
    for year in years:
        # Load historical data
        try:
            df = pd.read_parquet(f'../../data/historical_jobs_{year}.parquet')
            year_counts.append({'Year': year, 'Jobs Loaded': f"{len(df):,}"})
            all_data.append(df)
        except FileNotFoundError:
            year_counts.append({'Year': year, 'Jobs Loaded': "No data"})
        
        # Load current data if available and deduplicate
        try:
            current_df = pd.read_parquet(f'../../data/current_jobs_{year}.parquet')
            if len(current_df) > 0:
                # Deduplicate by usajobsControlNumber before combining
                existing_control_numbers = set(df['usajobsControlNumber']) if 'df' in locals() else set()
                new_current_jobs = current_df[~current_df['usajobsControlNumber'].isin(existing_control_numbers)]
                if len(new_current_jobs) > 0:
                    all_data.append(new_current_jobs)
                    year_counts[-1]['Jobs Loaded'] += f" + {len(new_current_jobs):,} current"
        except FileNotFoundError:
            pass
    
    # Create data loading summary table
    loading_summary = pd.DataFrame(year_counts)
    
    # Combine all years
    combined_df = pd.concat(all_data, ignore_index=True)
    
    # Convert dates with mixed format handling
    combined_df['positionOpenDate'] = pd.to_datetime(combined_df['positionOpenDate'], format='mixed')
    combined_df['year'] = combined_df['positionOpenDate'].dt.year
    combined_df['month'] = combined_df['positionOpenDate'].dt.month
    
    # Filter to only include data through June 2025
    combined_df = combined_df[
        (combined_df['year'] < 2025) | 
        ((combined_df['year'] == 2025) & (combined_df['month'] <= 6))
    ].copy()
    
    # Filter for National Park Service
    nps_df = combined_df[combined_df['hiringAgencyName'] == 'National Park Service'].copy()
    
    # Extract occupational series and categorize appointments
    nps_df['occupational_series'] = nps_df['JobCategories'].apply(extract_series)
    nps_df['appt_category'] = nps_df['appointmentType'].apply(categorize_appointment)
    
    # Create summary stats
    loading_stats = pd.DataFrame({
        'Metric': ['Total jobs loaded', 'National Park Service jobs', 'Data coverage'],
        'Value': [
            f"{len(combined_df):,}",
            f"{len(nps_df):,}",
            f"{len(year_counts)} years (2018-June 2025)"
        ]
    })
    
    return nps_df, loading_summary, loading_stats

# Load data
nps_df, loading_summary, loading_stats = load_nps_data()

# Create January-June filtered datasets for consistent analysis
nps_jan_jun = nps_df[nps_df['month'].between(1, 6)].copy()
nps_2025_jan_jun = nps_jan_jun[nps_jan_jun['year'] == 2025]
nps_historical_jan_jun = nps_jan_jun[nps_jan_jun['year'].between(2018, 2024)]

# Display data loading summary as Great Table
gt_loading_stats = (
    GT(loading_stats.reset_index(drop=True))
    .tab_header(
        title="Data Loading & Filtering Summary",
        subtitle="USAJobs Data Processing Results"
    )
    .cols_align(
        align="left",
        columns=["Metric"]
    )
    .cols_align(
        align="center",
        columns=["Value"]
    )
    .cols_width({
        "Metric": "60%",
        "Value": "40%"
    })
    .tab_options(quarto_disable_processing=True)
)
gt_loading_stats.show()

# Show appointment type categorization as Great Table
appt_breakdown = pd.DataFrame({
    'Appointment Type': nps_df['appointmentType'].value_counts().index,
    'Count': nps_df['appointmentType'].value_counts().values,
    'Category': [categorize_appointment(x) for x in nps_df['appointmentType'].value_counts().index]
})

gt_appt = (
    create_standard_gt_table(
        data=appt_breakdown,
        title="Appointment Type Categorization",
        subtitle="National Park Service Job Types (2018-2025)",
        align_left_cols=["Appointment Type", "Category"],
        align_center_cols=["Count"],
        col_widths={"Appointment Type": "45%", "Count": "20%", "Category": "35%"}
    )
    .fmt_number(columns=["Count"], sep_mark=",", decimals=0)
)
gt_appt.show()

```

# Monthly Hiring Heatmaps

```{python}
# Get current date to limit display
dt_info = get_current_datetime()
current_year = dt_info['year']
current_month = dt_info['month']

# Filter out future months - only show data through June 2025
def should_show_month(year, month):
    if year < 2025:
        return True
    elif year == 2025:
        return month <= 6  # Only show through June 2025
    else:
        return False

# Constants
MONTH_LABELS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

def create_heatmap_table(df_subset, title, subtitle=""):
    """Create a Great Tables heatmap-style table"""
    monthly_breakdown = df_subset.groupby(['year', 'month']).size().reset_index(name='job_count')
    monthly_pivot = monthly_breakdown.pivot(index='month', columns='year', values='job_count').fillna(0)
    
    # Mask future months
    for year in monthly_pivot.columns:
        for month in monthly_pivot.index:
            if not should_show_month(year, month):
                monthly_pivot.loc[month, year] = np.nan
    
    # Add month names
    monthly_pivot.index = MONTH_LABELS
    
    # Reset index to make month a column
    monthly_pivot_reset = monthly_pivot.reset_index()
    monthly_pivot_reset.columns.name = None
    monthly_pivot_reset = monthly_pivot_reset.rename(columns={'index': 'Month'})
    
    # Get year columns for formatting - convert to strings to ensure proper handling
    year_cols = [str(col) for col in monthly_pivot_reset.columns if str(col) != 'Month']
    
    # Create color scale values for the data
    max_val = monthly_pivot.max().max()
    
    # Rename columns to strings for Great Tables
    monthly_pivot_reset.columns = [str(col) for col in monthly_pivot_reset.columns]
    
    # Keep subtitle as-is for heatmaps (they already have repo link in footnote)
    
    gt_heatmap = (
        GT(monthly_pivot_reset)
        .tab_header(title=title, subtitle=subtitle)
        .tab_source_note(md("*Source: github.com/abigailhaddad/usajobs_historical*"))
        .fmt_number(columns=year_cols, decimals=0, sep_mark=",")
        .data_color(
            columns=year_cols,
            palette=["white", "orange", "darkred"],
            domain=[0, max_val],
            na_color="lightgray"
        )
        .cols_align(align="center", columns=year_cols)
        .cols_align(align="left", columns=["Month"])
        .tab_options(quarto_disable_processing=True)
    )
    
    gt_heatmap.show()

# Create heatmap summary data
def create_appointment_summary(df):
    """Create summary of job counts by appointment category"""
    permanent_count = len(df[df['appt_category'] == 'Permanent'])
    temp_count = len(df[df['appt_category'] == 'Term/Temporary'])
    total_count = len(df)
    
    return pd.DataFrame({
        'Category': ['All NPS Positions', 'Permanent Positions', 'Term/Temporary Positions'],
        'Job Count': [f"{total_count:,}", f"{permanent_count:,}", f"{temp_count:,}"],
        'Percentage': ["100%", f"{permanent_count/total_count*100:.0f}%", f"{temp_count/total_count*100:.0f}%"]
    })

heatmap_summary = create_appointment_summary(nps_df)
gt_heatmap_summary = create_standard_gt_table(
    data=heatmap_summary,
    title="Heatmap Categories Summary",
    subtitle="National Park Service Job Distribution by Appointment Type",
    align_left_cols=["Category"],
    align_center_cols=["Job Count", "Percentage"],
    col_widths={"Category": "50%", "Job Count": "25%", "Percentage": "25%"}
)
gt_heatmap_summary.show()

# 1. All NPS jobs
create_heatmap_table(nps_df, 
                    "National Park Service - All USAJobs Postings by Month and Year",
                    "")

# 2. Permanent positions only
permanent_df = nps_df[nps_df['appt_category'] == 'Permanent']
create_heatmap_table(permanent_df, 
                    "National Park Service - Permanent USAJobs Positions",
                    "")

# 3. Term/Temporary positions only  
temp_df = nps_df[nps_df['appt_category'] == 'Term/Temporary']
create_heatmap_table(temp_df, 
                    "National Park Service - Term/Temporary USAJobs Positions",
                    "")
```

# 2025 vs Previous Years: What's Being Hired Less

```{python}
def analyze_occupation_changes():
    """Analyze Jan-Jun 2018-2024 vs Jan-Jun 2025 occupational series changes"""
    # Load occupation mapping
    occ_mapping = pd.read_csv('../DTocc.txt')
    occ_dict = dict(zip(occ_mapping['OCC'].astype(str).str.zfill(4), occ_mapping['OCCT']))
    
    # Create summary table first
    # Calculate average for historical period
    historical_avg = len(nps_historical_jan_jun) / 7  # 7 years (2018-2024)
    pct_change = ((len(nps_2025_jan_jun) - historical_avg) / historical_avg) * 100
    summary_data = pd.DataFrame({
        'Period': ['2025 Jan-Jun jobs', 'Historical Jan-Jun average (2018-2024)', 'Change'],
        'Count': [f"{len(nps_2025_jan_jun):,}", f"{historical_avg:,.0f}", f"{pct_change:.0f}%"]
    })
    
    gt_scope = create_standard_gt_table(
        data=summary_data,
        title="Analysis Scope",
        subtitle="January-June Comparison Only",
        align_left_cols=["Period"],
        align_center_cols=["Count"],
        col_widths={"Period": "70%", "Count": "30%"}
    )
    gt_scope.show()
    
    # Get 2025 Jan-Jun counts by occupational series
    occ_2025 = nps_2025_jan_jun['occupational_series'].value_counts()
    
    # Get historical Jan-Jun average by occupational series
    occ_historical = nps_historical_jan_jun.groupby(['year', 'occupational_series']).size().reset_index(name='count')
    occ_historical_avg = occ_historical.groupby('occupational_series')['count'].mean()
    
    # Compare and find biggest changes
    comparison_data = []
    all_series = set(occ_2025.index) | set(occ_historical_avg.index)
    
    for series in all_series:
        if pd.notna(series) and series != 'Unknown':
            count_2025 = occ_2025.get(series, 0)
            avg_historical = occ_historical_avg.get(series, 0)
            
            if avg_historical >= 2:  # Only meaningful changes
                difference = count_2025 - avg_historical
                pct_change = ((count_2025 - avg_historical) / avg_historical) * 100 if avg_historical > 0 else 0
                occ_title = occ_dict.get(series, f"Series {series}")
                comparison_data.append({
                    'Occupation': occ_title,
                    'Historical\nAvg': round(avg_historical),
                    '2025\nActual': count_2025,
                    'Change': round(difference),
                    '% Change': round(pct_change, 0)
                })
    
    # Convert to DataFrame and sort by absolute change (biggest changes first)
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df['abs_change'] = abs(comparison_df['Change'])
    comparison_df = comparison_df.sort_values('abs_change', ascending=False)
    comparison_df = comparison_df.drop('abs_change', axis=1)
    
    return comparison_df

# Analyze changes
changes_df = analyze_occupation_changes()

# Display top 10
gt_df_top10 = changes_df.head(10).reset_index(drop=True)

# Create wider table for occupation names
gt_table_top10 = (
    GT(gt_df_top10.reset_index(drop=True))
    .tab_header(
        title="National Park Service: Top 10 Occupations by Biggest Changes",
        subtitle="Jan-June 2018-2024 vs Jan-June 2025 | USAJobs Historical Data"
    )
    .tab_source_note(md("*Source: github.com/abigailhaddad/usajobs_historical*"))
    .cols_align(align="left", columns=["Occupation"])
    .cols_align(align="center", columns=["Historical\nAvg", "2025\nActual", "Change", "% Change"])
    .cols_width({
        "Occupation": "45%",
        "Historical\nAvg": "18%",
        "2025\nActual": "12%", 
        "Change": "12%",
        "% Change": "13%"
    })
    .fmt_number(columns=["Historical\nAvg", "Change"], decimals=0)
    .fmt_number(columns=["2025\nActual"], decimals=0)
    .fmt_number(columns=["% Change"], decimals=0, pattern="{x}%")
    .data_color(
        columns=["% Change"],
        palette=["red", "white", "green"],
        domain=[-100, 50]
    )
    .tab_options(quarto_disable_processing=True)
)

gt_table_top10.show()

# Show expandable section for all results if there are more than 10
if len(changes_df) > 10:
    from IPython.display import display, HTML
    
    # Create collapsible HTML section
    expand_html = f"""
    <details style="margin-top: 20px;">
    <summary style="cursor: pointer; font-weight: bold; padding: 10px; background-color: #f0f0f0; border-radius: 5px;">
    📋 Show all {len(changes_df)} occupations
    </summary>
    <div style="margin-top: 10px;">
    """
    
    display(HTML(expand_html))
    
    # Display all results
    gt_df_all = changes_df.reset_index(drop=True)
    
    # Create wider table for all occupations  
    gt_table_all = (
        GT(gt_df_all.reset_index(drop=True))
        .tab_header(
            title="National Park Service: All Occupational Changes",
            subtitle="Jan-June 2018-2024 vs Jan-June 2025 | USAJobs Historical Data"
        )
        .tab_source_note(md("*Source: github.com/abigailhaddad/usajobs_historical*"))
        .cols_align(align="left", columns=["Occupation"])
        .cols_align(align="center", columns=["Historical\nAvg", "2025\nActual", "Change", "% Change"])
        .cols_width({
            "Occupation": "45%",
            "Historical\nAvg": "18%",
            "2025\nActual": "12%", 
            "Change": "12%",
            "% Change": "13%"
        })
        .fmt_number(columns=["Historical\nAvg", "Change"], decimals=0)
        .fmt_number(columns=["2025\nActual"], decimals=0)
        .fmt_number(columns=["% Change"], decimals=0, pattern="{x}%")
        .data_color(
            columns=["% Change"],
            palette=["red", "white", "green"],
            domain=[-100, 50]
        )
        .tab_options(quarto_disable_processing=True)
    )
    
    gt_table_all.show()
    
    display(HTML("</div></details>"))
```

# Are There More Group Announcements in 2025?

```{python}
def categorize_openings(opening_val):
    """Categorize openings (MANY=Many, FEW=Few, etc.)"""
    opening_str = str(opening_val).upper()
    if opening_str in ['MANY', 'FEW', 'SEVERAL']:
        return opening_str.title()
    else:
        try:
            return str(int(float(opening_val)))
        except:
            return 'Other'

def analyze_opening_types():
    """Analyze opening types comparison between Jan-June 2018-2024 and Jan-June 2025"""
    # Use pre-filtered January-June datasets and filter for jobs with opening data
    openings_2025 = nps_2025_jan_jun[nps_2025_jan_jun['totalOpenings'].notna()].copy()
    openings_historical = nps_historical_jan_jun[nps_historical_jan_jun['totalOpenings'].notna()].copy()
    
    # Apply categorization
    openings_2025.loc[:, 'opening_category'] = openings_2025['totalOpenings'].apply(categorize_openings)
    openings_historical.loc[:, 'opening_category'] = openings_historical['totalOpenings'].apply(categorize_openings)
    
    # Get top 10 categories for each period
    hist_top10 = openings_historical['opening_category'].value_counts().head(10)
    curr_top10 = openings_2025['opening_category'].value_counts().head(10)
    
    # Calculate percentages
    hist_pcts = (hist_top10 / len(openings_historical) * 100).round(0)
    curr_pcts = (curr_top10 / len(openings_2025) * 100).round(0)
    
    # Create simple comparison table
    comparison_data = []
    all_categories = set(hist_top10.index) | set(curr_top10.index)

    
    for category in sorted(all_categories):
        hist_pct = hist_pcts.get(category, 0)
        curr_pct = curr_pcts.get(category, 0)
        comparison_data.append({
            'Total Openings': category,
            'Historical\n%': hist_pct,
            '2025\n%': curr_pct,
            'Change': round(curr_pct - hist_pct, 0)
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df = comparison_df.sort_values('Historical\n%', ascending=False)
    
    # Display with Great Tables
    gt_openings_df = comparison_df.reset_index(drop=True)
    
    gt_openings = (
        create_standard_gt_table(
            data=gt_openings_df,
            title="National Park Service: Top 10 Total Openings Comparison",
            subtitle="Jan-June 2018-2024 vs Jan-June 2025",
            align_left_cols=["Total Openings"],
            align_center_cols=["Historical\n%", "2025\n%", "Change"],
            col_widths={
                "Total Openings": "35%",
                "Historical\n%": "22%",
                "2025\n%": "22%",
                "Change": "21%"
            }
        )
        .fmt_number(
            columns=["Historical\n%", "2025\n%", "Change"],
            decimals=0,
            pattern="{x}%"
        )
        .data_color(
            columns=["Change"],
            palette=["red", "white", "green"],
            domain=[-25, 25]
        )
    )
    
    gt_openings.show()

# Run the analysis
analyze_opening_types()

```

---
```{python}
from IPython.display import display, Markdown
dt_info = get_current_datetime()
display(Markdown(f"*Analysis generated on {dt_info['formatted']}*"))
```