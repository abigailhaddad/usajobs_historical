---
title: "Analysis of 'Advancing President's Executive Orders' Question in Federal Job Questionnaires"
output-file: index.html
format: 
  html:
    code-fold: true
    toc: true
execute:
  warning: false
  message: false
---

```{python}
import pandas as pd
import os
from pathlib import Path
import re
from collections import Counter
from IPython.display import HTML, display
from datetime import datetime

# Define paths
BASE_DIR = Path('../..')
QUESTIONNAIRE_DIR = Path('..')
DATA_DIR = BASE_DIR / 'data'
RAW_QUESTIONNAIRES_DIR = QUESTIONNAIRE_DIR / 'raw_questionnaires'

# Set pandas display options for better text wrapping
pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_rows', None)
```

## Data Coverage

```{python}
# Load questionnaire links to get date range
links_df_preview = pd.read_csv(QUESTIONNAIRE_DIR / 'questionnaire_links.csv')

# Convert dates and get range
if 'position_open_date' in links_df_preview.columns:
    links_df_preview['position_open_date'] = pd.to_datetime(links_df_preview['position_open_date'], format='mixed')
    
    # Get date range of analyzed jobs
    earliest_date = links_df_preview['position_open_date'].min()
    latest_date = links_df_preview['position_open_date'].max()
    
    print(f"ðŸ“… Analyzing questionnaires from federal job postings")
    print(f"   Date range: {earliest_date.strftime('%B %d, %Y')} to {latest_date.strftime('%B %d, %Y')}")
else:
    print("ðŸ“… Questionnaire analysis (date information not available)")
```

## Utility Functions

```{python}
def extract_file_id_from_url(url):
    """Extract questionnaire file ID and prefix from URL"""
    if pd.isna(url):
        return None, None
        
    if 'usastaffing.gov' in url:
        match = re.search(r'ViewQuestionnaire/(\d+)', url)
        file_id = match.group(1) if match else 'unknown'
        prefix = 'usastaffing'
    elif 'monstergovt.com' in url:
        match = re.search(r'jnum=(\d+)', url)
        if not match:
            match = re.search(r'J=(\d+)', url)
        file_id = match.group(1) if match else 'unknown'
        prefix = 'monster'
    else:
        file_id = str(hash(url))[:8]
        prefix = 'other'
    
    return file_id, prefix

def extract_questionnaire_id(url):
    """Extract questionnaire ID from URL (for USAStaffing URLs only)"""
    if pd.isna(url):
        return None
    match = re.search(r'/ViewQuestionnaire/(\d+)', url)
    if match:
        return match.group(1)
    return None

def get_questionnaire_filepath(url):
    """Get the expected filepath for a questionnaire URL"""
    file_id, prefix = extract_file_id_from_url(url)
    if file_id and prefix:
        return RAW_QUESTIONNAIRES_DIR / f'{prefix}_{file_id}.txt'
    return None

def check_questionnaire_exists(url):
    """Check if a questionnaire file exists for the given URL"""
    filepath = get_questionnaire_filepath(url)
    return filepath and filepath.exists()

def build_usajobs_url(control_number):
    """Build USAJobs URL from control number"""
    if pd.notna(control_number):
        return f"https://www.usajobs.gov/job/{control_number}"
    return None

def create_html_link(url, text="Link"):
    """Create HTML link from URL"""
    if url and url != "N/A":
        return f'<a href="{url}" target="_blank">{text}</a>'
    return "N/A"

def format_occupation(series, name):
    """Format occupation series and name"""
    if pd.notna(series) and pd.notna(name):
        return f"{series} - {name}"
    elif pd.notna(series):
        return str(series)
    return "Unknown"

def style_table(df):
    """Apply consistent styling to dataframe for HTML display"""
    return df.style.set_properties(**{
        'text-align': 'left',
        'white-space': 'pre-wrap',
        'word-wrap': 'break-word'
    }).set_table_styles([
        {'selector': 'td', 'props': [('max-width', '300px')]}
    ])

def load_questionnaire_links():
    """Load the questionnaire links CSV"""
    csv_path = QUESTIONNAIRE_DIR / 'questionnaire_links.csv'
    if not csv_path.exists():
        raise FileNotFoundError(f"questionnaire_links.csv not found at {csv_path}")
    
    df = pd.read_csv(csv_path)
    print(f"Loaded {len(df):,} questionnaire links")
    return df

def check_executive_order_mentions(questionnaire_dir=RAW_QUESTIONNAIRES_DIR):
    """Check which questionnaires mention the specific executive order question"""
    mentions = {}
    # Look for the specific question about advancing President's Executive Orders
    pattern = re.compile(r"How would you help advance the President's Executive Orders and policy priorities in this role\?", re.IGNORECASE)
    
    if not questionnaire_dir.exists():
        print(f"Warning: {questionnaire_dir} does not exist")
        return mentions
    
    txt_files = list(questionnaire_dir.glob('*.txt'))
    print(f"Found {len(txt_files):,} scraped questionnaire files")
    
    for txt_file in txt_files:
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Find all mentions of the specific question
            eo_mentions = pattern.findall(content)
            if eo_mentions:
                # Extract questionnaire ID from filename
                file_id = txt_file.stem.split('_')[1]
                mentions[file_id] = len(eo_mentions)
        except Exception as e:
            print(f"Error reading {txt_file}: {e}")
    
    return mentions

def get_scraped_questionnaire_ids():
    """Get set of all scraped questionnaire IDs"""
    scraped_ids = set()
    
    for txt_file in RAW_QUESTIONNAIRES_DIR.glob('*.txt'):
        file_id = txt_file.stem.split('_')[1]
        scraped_ids.add(file_id)
    
    return scraped_ids

def prepare_job_table_data(df):
    """Prepare dataframe for job listing table display"""
    table_df = df.copy()
    
    # Create display columns
    table_df['occupation_display'] = table_df.apply(
        lambda x: format_occupation(x.get('occupation_series'), x.get('occupation_name')), 
        axis=1
    )
    
    # Create clickable links
    job_urls = table_df['usajobs_control_number'].apply(build_usajobs_url)
    table_df['job_posting_link'] = job_urls.apply(lambda x: create_html_link(x, "USAJobs Link"))
    table_df['questionnaire_link'] = table_df['questionnaire_url'].apply(
        lambda x: create_html_link(x, "Questionnaire")
    )
    
    # Format dates - handle mixed formats
    table_df['open_date'] = pd.to_datetime(table_df['position_open_date'], format='mixed').dt.strftime('%m/%d/%Y')
    table_df['close_date'] = pd.to_datetime(table_df['position_close_date'], format='mixed').dt.strftime('%m/%d/%Y')
    
    # Select and rename columns
    result = table_df[['position_title', 'occupation_display', 'hiring_agency', 
                       'position_location', 'grade_code', 'service_type', 'open_date', 'close_date',
                       'job_posting_link', 'questionnaire_link']].copy()
    result.columns = ['Position_Title', 'Occupation', 'Agency', 'Location', 
                      'Grade', 'Service', 'Open', 'Close', 'Job_Posting', 'Questionnaire']
    
    return result.sort_values('Position_Title')
```

## Load and Process Data

```{python}
# Load questionnaire links
links_df = load_questionnaire_links()

# Service type is now already in the CSV from extraction
print(f"Service type already loaded from CSV")

# Check for the specific executive order question
eo_mentions = check_executive_order_mentions()
print(f"\nFound {len(eo_mentions):,} questionnaires with the 'advancing President's Executive Orders' question")

# Get all scraped IDs
scraped_ids = get_scraped_questionnaire_ids()

# Add questionnaire ID and executive order flags
links_df['questionnaire_id'] = links_df['questionnaire_url'].apply(extract_questionnaire_id)
links_df['has_executive_order'] = links_df['questionnaire_id'].isin(eo_mentions)
links_df['eo_mention_count'] = links_df['questionnaire_id'].map(eo_mentions).fillna(0).astype(int)

# Filter to only scraped questionnaires
scraped_df = links_df[links_df['questionnaire_id'].isin(scraped_ids)].copy()

print(f"\nTotal scraped questionnaires: {len(scraped_df):,}")
print(f"Records with executive order mentions: {scraped_df['has_executive_order'].sum():,}")
```

## Executive Order Analysis

### Overall Statistics

```{python}
# Calculate overall statistics for scraped questionnaires only
total_scraped = len(scraped_df)
total_with_eo = scraped_df['has_executive_order'].sum()
percentage_with_eo = (total_with_eo / total_scraped * 100) if total_scraped > 0 else 0

print(f"Total scraped questionnaires: {total_scraped:,}")
print(f"Questionnaires with the specific EO question: {total_with_eo:,}")
print(f"Percentage with the EO question: {percentage_with_eo:.1f}%")
```

### By Service Type

```{python}
# Analyze by service type for jobs with EO question
eo_jobs = scraped_df[scraped_df['has_executive_order']].copy()

if 'service_type' in eo_jobs.columns:
    service_summary = eo_jobs['service_type'].value_counts()
    print("Service types for jobs with the EO question:")
    for service, count in service_summary.items():
        print(f"  {service}: {count} ({count/len(eo_jobs)*100:.1f}%)")
else:
    print("Service type information not available")
```

### By Grade Level

```{python}
# Analyze by grade level for jobs with EO question
if 'grade_code' in eo_jobs.columns:
    # Clean and standardize grade codes
    eo_jobs['grade_clean'] = eo_jobs['grade_code'].fillna('Not Specified')
    grade_summary = eo_jobs['grade_clean'].value_counts().head(20)
    print(f"Top 20 grade levels for jobs with the EO question:")
    for grade, count in grade_summary.items():
        print(f"  {grade}: {count}")
else:
    print("Grade level information not available")
```

### By Agency

```{python}
# Analyze by agency
agency_summary = scraped_df.groupby('hiring_agency').agg({
    'questionnaire_id': 'count',
    'has_executive_order': 'sum'
}).reset_index()

agency_summary.columns = ['Agency', 'Total_Scraped', 'Count_With_EO']
agency_summary['Percent'] = (agency_summary['Count_With_EO'] / agency_summary['Total_Scraped'] * 100).round(0).astype(int)

# Sort by count with EO mentions
agency_summary = agency_summary.sort_values('Count_With_EO', ascending=False)

# Display agency table - only show agencies that have the question
agency_with_eo = agency_summary[agency_summary['Count_With_EO'] > 0]
print(f"Agencies using the 'advancing President's Executive Orders' question ({len(agency_with_eo)} agencies):")
agency_with_eo.set_index('Agency')[['Total_Scraped', 'Count_With_EO', 'Percent']]
```

### By Occupation Series

```{python}
# Create occupation display for grouping
scraped_df['occupation_display'] = scraped_df.apply(
    lambda x: format_occupation(x['occupation_series'], x['occupation_name']), 
    axis=1
)

# Analyze by occupation series
occupation_summary = scraped_df.groupby('occupation_display').agg({
    'questionnaire_id': 'count',
    'has_executive_order': 'sum'
}).reset_index()

occupation_summary.columns = ['Occupation', 'Total_Scraped', 'Count_With_EO']
occupation_summary['Percent'] = (occupation_summary['Count_With_EO'] / occupation_summary['Total_Scraped'] * 100).round(0).astype(int)

# Sort by count with EO mentions
occupation_summary = occupation_summary.sort_values('Count_With_EO', ascending=False)

# Display occupation series table - only show occupations that have the question
occupation_with_eo = occupation_summary[occupation_summary['Count_With_EO'] > 0]
print(f"Occupation series using the 'advancing President's Executive Orders' question ({len(occupation_with_eo)} occupations):")
occupation_with_eo.set_index('Occupation')[['Total_Scraped', 'Count_With_EO', 'Percent']]
```

### By Month Posted

```{python}
# Extract month from position_open_date (when the job was actually posted)
scraped_df['posted_month'] = pd.to_datetime(scraped_df['position_open_date'], format='mixed').dt.to_period('M')

# Remove any rows with missing dates
scraped_with_dates = scraped_df.dropna(subset=['posted_month'])

# Analyze by month posted
month_summary = scraped_with_dates.groupby('posted_month').agg({
    'questionnaire_id': 'count',
    'has_executive_order': 'sum'
}).reset_index()

month_summary.columns = ['Month_Posted', 'Total_Scraped', 'Count_With_EO']
month_summary['Percent'] = (month_summary['Count_With_EO'] / month_summary['Total_Scraped'] * 100).round(0).astype(int)

# Sort by month
month_summary = month_summary.sort_values('Month_Posted')

# Convert Period to string for display
month_summary['Month_Posted'] = month_summary['Month_Posted'].astype(str)

# Display month table - only show months that have the question
month_with_eo = month_summary[month_summary['Count_With_EO'] > 0]
print(f"\nMonths with jobs using the 'advancing President's Executive Orders' question ({len(month_with_eo)} months):")
print(f"(Based on {len(scraped_with_dates):,} scraped questionnaires with valid posting dates)")
month_with_eo.set_index('Month_Posted')[['Total_Scraped', 'Count_With_EO', 'Percent']]
```

### Job Postings with the Executive Order Question

```{python}
# Get all jobs with EO mentions
eo_jobs = scraped_df[scraped_df['has_executive_order']].copy()

# Prepare the table
detail_table = prepare_job_table_data(eo_jobs)

print(f"All {len(detail_table)} job postings with the 'advancing President's Executive Orders' question:")

# Display styled table
styled_table = style_table(detail_table)
HTML(styled_table.to_html(escape=False, index=False))
```

## Summary

```{python}
#| output: asis
summary_text = f"""
This analysis examined **{total_scraped:,}** scraped federal job questionnaires and found that **{total_with_eo:,}** ({percentage_with_eo:.1f}%) contain the specific question: "How would you help advance the President's Executive Orders and policy priorities in this role?"

The agencies and positions that include this question in their assessment are shown in the tables above.
"""
print(summary_text)
```

## Failed Questionnaire Scrapes

```{python}
# Find questionnaires that failed to scrape
missing_questionnaires = []

for idx, row in links_df.iterrows():
    if not check_questionnaire_exists(row['questionnaire_url']):
        missing_questionnaires.append(row)

if missing_questionnaires:
    # Convert to dataframe
    failed_df = pd.DataFrame(missing_questionnaires)
    
    # Prepare the table
    failed_table = prepare_job_table_data(failed_df)
    
    print(f"Found {len(failed_table)} questionnaires that failed to scrape:")
    
    # Display styled table
    styled_failed = style_table(failed_table)
    display(HTML(styled_failed.to_html(escape=False, index=False)))
else:
    print("All questionnaires were successfully scraped!")
```

