---
title: "Analysis of 'Advancing President's Executive Orders' Question in Federal Job Questionnaires"
output-file: index.html
format: 
  html:
    code-fold: true
    toc: true
    theme: cosmo
    highlight-style: github
    df-print: paged
    embed-resources: true
    html-table-processing: none
execute:
  warning: false
  message: false
---

```{python}
import pandas as pd
import os
from pathlib import Path
import re
from collections import Counter
from datetime import datetime
from great_tables import GT

# Define paths
BASE_DIR = Path('../..')
QUESTIONNAIRE_DIR = Path('..')
DATA_DIR = BASE_DIR / 'data'
RAW_QUESTIONNAIRES_DIR = QUESTIONNAIRE_DIR / 'raw_questionnaires'

# Set pandas display options for better text wrapping
pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_rows', None)
```

## Data Coverage

```{python}
# Load questionnaire links to get date range
links_df_preview = pd.read_csv(QUESTIONNAIRE_DIR / 'questionnaire_links.csv')

# Convert dates and get range
if 'position_open_date' in links_df_preview.columns:
    links_df_preview['position_open_date'] = pd.to_datetime(links_df_preview['position_open_date'], format='mixed')
    
    # Get date range of analyzed jobs
    earliest_date = links_df_preview['position_open_date'].min()
    latest_date = links_df_preview['position_open_date'].max()
    
    print(f"ðŸ“… Analyzing questionnaires from federal job postings")
    print(f"   Date range: {earliest_date.strftime('%B %d, %Y')} to {latest_date.strftime('%B %d, %Y')}")
else:
    print("ðŸ“… Questionnaire analysis (date information not available)")
```

## Core Functions

```{python}
def calculate_eo_stats(df, group_column, top_n=None):
    """Calculate EO question statistics for any grouping column"""
    stats = df.groupby(group_column).agg({
        'has_executive_order': ['sum', 'count']
    }).reset_index()
    
    stats.columns = [group_column, 'Has EO Question', 'Total Scraped']
    stats['% With EO Question'] = (stats['Has EO Question'] / stats['Total Scraped'] * 100).round(1)
    
    if top_n:
        stats = stats.nlargest(top_n, 'Total Scraped')
    
    return stats.sort_values('Total Scraped', ascending=False).reset_index(drop=True)

def check_executive_order_mentions(questionnaire_dir=RAW_QUESTIONNAIRES_DIR):
    """Check which questionnaires mention the specific executive order question"""
    mentions = {}
    pattern = re.compile(r"How would you help advance the President's Executive Orders and policy priorities in this role\?", re.IGNORECASE)
    
    if not questionnaire_dir.exists():
        print(f"Warning: {questionnaire_dir} does not exist")
        return mentions
    
    txt_files = list(questionnaire_dir.glob('*.txt'))
    print(f"Found {len(txt_files):,} scraped questionnaire files")
    
    for txt_file in txt_files:
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            if pattern.search(content):
                file_id = txt_file.stem.split('_')[1]
                mentions[file_id] = 1
        except Exception as e:
            print(f"Error reading {txt_file}: {e}")
    
    return mentions
```

## Load and Process Data

```{python}
# Load questionnaire links
links_df = pd.read_csv(QUESTIONNAIRE_DIR / 'questionnaire_links.csv')
print(f"Loaded {len(links_df):,} questionnaire links")

# Check for the specific executive order question
eo_mentions = check_executive_order_mentions()
print(f"\nFound {len(eo_mentions):,} questionnaires with the 'advancing President's Executive Orders' question")

# Get all scraped IDs
scraped_ids = set()
for txt_file in RAW_QUESTIONNAIRES_DIR.glob('*.txt'):
    file_id = txt_file.stem.split('_')[1]
    scraped_ids.add(file_id)

# Add questionnaire ID and executive order flags
# Extract ID from both USAStaffing and Monster URLs
def extract_questionnaire_id(url):
    """Extract questionnaire ID from either USAStaffing or Monster URLs"""
    if 'usastaffing.gov' in url:
        match = re.search(r'/ViewQuestionnaire/(\d+)', url)
        return match.group(1) if match else None
    elif 'monstergovt.com' in url:
        match = re.search(r'jnum=(\d+)', url)
        if not match:
            match = re.search(r'J=(\d+)', url)
        return match.group(1) if match else None
    return None

links_df['questionnaire_id'] = links_df['questionnaire_url'].apply(extract_questionnaire_id)
links_df['has_executive_order'] = links_df['questionnaire_id'].isin(eo_mentions)

# Filter to only scraped questionnaires
scraped_df = links_df[links_df['questionnaire_id'].isin(scraped_ids)].copy()

print(f"\nTotal scraped questionnaires: {len(scraped_df):,}")
print(f"Records with executive order mentions: {scraped_df['has_executive_order'].sum():,}")
```

## Executive Order Analysis

### Overall Statistics

```{python}
# Calculate overall statistics for scraped questionnaires only
total_scraped = len(scraped_df)
total_with_eo = scraped_df['has_executive_order'].sum()
percentage_with_eo = (total_with_eo / total_scraped * 100) if total_scraped > 0 else 0

print(f"Total scraped questionnaires: {total_scraped:,}")
print(f"Questionnaires with the specific EO question: {total_with_eo:,}")
print(f"Percentage with the EO question: {percentage_with_eo:.1f}%")
```

### Service Type Analysis

```{python}
# Analyze EO question usage by service type
if 'service_type' in scraped_df.columns:
    service_stats = calculate_eo_stats(scraped_df, 'service_type')
    
    gt_service = (
        GT(service_stats)
        .tab_header(title="Service Type Analysis")
        .tab_options(row_striping_include_table_body=False)
        .opt_row_striping(row_striping=False)
        .fmt_number(columns=['Has EO Question', 'Total Scraped'], decimals=0)
        .fmt_number(columns=['% With EO Question'], decimals=1)
        .data_color(
            columns=['% With EO Question'],
            palette="Blues"
        )
    )
    gt_service.show()
```

### Grade Level Analysis (Most Frequent Grades)

```{python}
# Analyze EO question usage by grade level - top 10
if 'grade_code' in scraped_df.columns:
    scraped_df['grade_clean'] = scraped_df['grade_code'].fillna('Not Specified')
    grade_stats = calculate_eo_stats(scraped_df, 'grade_clean', top_n=10)
    
    gt_grade = (
        GT(grade_stats)
        .tab_header(title="Grade Level Analysis (Most Frequent Grades)")
        .tab_options(row_striping_include_table_body=False)
        .opt_row_striping(row_striping=False)
        .fmt_number(columns=['Has EO Question', 'Total Scraped'], decimals=0)
        .fmt_number(columns=['% With EO Question'], decimals=1)
        .data_color(
            columns=['% With EO Question'],
            palette="Blues"
        )
    )
    gt_grade.show()
```

### Geographic Analysis (Top 10 Locations)

```{python}
# Analyze by full location names - top 10
if 'position_location' in scraped_df.columns:
    location_stats = calculate_eo_stats(scraped_df, 'position_location', top_n=10)
    
    gt_location = (
        GT(location_stats)
        .tab_header(title="Geographic Analysis (Top 10 Locations)")
        .tab_options(row_striping_include_table_body=False)
        .opt_row_striping(row_striping=False)
        .fmt_number(columns=['Has EO Question', 'Total Scraped'], decimals=0)
        .fmt_number(columns=['% With EO Question'], decimals=1)
        .data_color(
            columns=['% With EO Question'],
            palette="Blues"
        )
    )
    gt_location.show()
```



### Agency Analysis

```{python}
# Analyze by agency - top 20
agency_stats = calculate_eo_stats(scraped_df, 'hiring_agency', top_n=20)

gt_agency = (
    GT(agency_stats)
    .tab_header(title="Agency Analysis (Top 20)")
    .tab_options(row_striping_include_table_body=False)
    .opt_row_striping(row_striping=False)
    .fmt_number(columns=['Has EO Question', 'Total Scraped'], decimals=0)
    .fmt_number(columns=['% With EO Question'], decimals=1)
    .data_color(
        columns=['% With EO Question'],
        palette="Blues"
    )
)
gt_agency.show()
```

### Occupation Series Analysis

```{python}
# Create occupation display
scraped_df['occupation_display'] = scraped_df['occupation_series'].astype(str) + ' - ' + scraped_df['occupation_name'].fillna('Unknown')

# Analyze by occupation - top 20
occupation_stats = calculate_eo_stats(scraped_df, 'occupation_display', top_n=20)

gt_occupation = (
    GT(occupation_stats)
    .tab_header(title="Occupation Series Analysis (Top 20)")
    .tab_options(row_striping_include_table_body=False)
    .opt_row_striping(row_striping=False)
    .fmt_number(columns=['Has EO Question', 'Total Scraped'], decimals=0)
    .fmt_number(columns=['% With EO Question'], decimals=1)
    .data_color(
        columns=['% With EO Question'],
        palette="Blues"
    )
)
gt_occupation.show()
```

### Executive Order Question Usage by Job Opening Date

```{python}
# Convert dates if not already done
if 'position_open_date' in scraped_df.columns and 'open_date' not in scraped_df.columns:
    scraped_df['open_date'] = pd.to_datetime(scraped_df['position_open_date'], format='mixed', errors='coerce')
if 'position_close_date' in scraped_df.columns and 'close_date' not in scraped_df.columns:
    scraped_df['close_date'] = pd.to_datetime(scraped_df['position_close_date'], format='mixed', errors='coerce')

# Create weekly percentage heatmap
if 'open_date' in scraped_df.columns:
    # Get jobs with valid open dates
    jobs_with_dates = scraped_df[scraped_df['open_date'].notna()].copy()
    
    # Extract date components
    jobs_with_dates['year'] = jobs_with_dates['open_date'].dt.year
    jobs_with_dates['month'] = jobs_with_dates['open_date'].dt.month
    jobs_with_dates['month_name'] = jobs_with_dates['open_date'].dt.strftime('%B %Y')
    jobs_with_dates['week_of_month'] = jobs_with_dates['open_date'].dt.day.apply(lambda d: (d-1)//7 + 1)
    
    # Group by month and week to calculate percentages
    weekly_stats = jobs_with_dates.groupby(['year', 'month', 'month_name', 'week_of_month']).agg({
        'questionnaire_id': 'count',
        'has_executive_order': 'sum'
    }).reset_index()
    
    # Only calculate percentage where there are jobs (avoid 0.0 for empty weeks)
    weekly_stats['percentage'] = weekly_stats.apply(
        lambda row: round(row['has_executive_order'] / row['questionnaire_id'] * 100, 1) if row['questionnaire_id'] > 0 else None,
        axis=1
    )
    
    # Pivot to create heatmap format - don't fill missing values
    heatmap_data = weekly_stats.pivot_table(
        index=['year', 'month', 'month_name'],
        columns='week_of_month',
        values='percentage',
        aggfunc='first'  # Just take the value, don't aggregate
    ).reset_index()
    
    # Sort by year and month
    heatmap_data = heatmap_data.sort_values(['year', 'month'])
    
    # Rename columns
    heatmap_data.columns.name = None
    column_mapping = {1: 'Week 1', 2: 'Week 2', 3: 'Week 3', 4: 'Week 4', 5: 'Week 5'}
    heatmap_data = heatmap_data.rename(columns=column_mapping)
    
    # Calculate monthly totals
    monthly_totals = weekly_stats.groupby(['year', 'month', 'month_name']).agg({
        'questionnaire_id': 'sum',
        'has_executive_order': 'sum'
    }).reset_index()
    monthly_totals['monthly_percentage'] = (monthly_totals['has_executive_order'] / monthly_totals['questionnaire_id'] * 100).round(1)
    
    # Keep only month name for display
    display_data = heatmap_data[['month_name', 'Week 1', 'Week 2', 'Week 3', 'Week 4', 'Week 5']].copy()
    
    # Merge with monthly totals
    display_data = display_data.merge(
        monthly_totals[['month_name', 'questionnaire_id', 'has_executive_order', 'monthly_percentage']],
        on='month_name',
        how='left'
    )
    
    # Rename columns
    display_data = display_data.rename(columns={
        'month_name': 'Month',
        'questionnaire_id': 'Total Jobs',
        'has_executive_order': 'With EO',
        'monthly_percentage': 'Month %'
    })
    
    # Reorder columns to put Month % after Week 5
    display_data = display_data[['Month', 'Week 1', 'Week 2', 'Week 3', 'Week 4', 'Week 5', 'Month %', 'Total Jobs', 'With EO']]
    
    # Handle missing weeks - ensure all week columns exist
    for week in ['Week 1', 'Week 2', 'Week 3', 'Week 4', 'Week 5']:
        if week not in display_data.columns:
            display_data[week] = None
    
    # Find max percentage across all percentage columns for consistent scaling
    percentage_cols = ['Week 1', 'Week 2', 'Week 3', 'Week 4', 'Week 5', 'Month %']
    max_percentage = display_data[percentage_cols].max().max()
    
    # Create GT table
    gt_table = (
        GT(display_data)
        .tab_header(
            title="Executive Order Question Usage by Job Opening Date",
            subtitle="Weekly and monthly percentages of jobs posted with the EO question"
        )
        .opt_row_striping(row_striping=False)
        .fmt_number(
            columns=['Total Jobs', 'With EO'],
            decimals=0
        )
        .fmt_number(
            columns=['Week 1', 'Week 2', 'Week 3', 'Week 4', 'Week 5', 'Month %'],
            decimals=1
        )
        .sub_missing(missing_text="â€”")
        .data_color(
            columns=['Week 1', 'Week 2', 'Week 3', 'Week 4', 'Week 5', 'Month %'],
            palette="Blues",
            domain=[0, max_percentage]
        )
    )
    
    gt_table.show()
```

### Job Postings with the Executive Order Question

```{python}
# Get all jobs with EO mentions
eo_jobs = scraped_df[scraped_df['has_executive_order']].copy()

# Format dates
eo_jobs['open_date'] = pd.to_datetime(eo_jobs['position_open_date'], format='mixed').dt.strftime('%m/%d/%Y')
eo_jobs['close_date'] = pd.to_datetime(eo_jobs['position_close_date'], format='mixed').dt.strftime('%m/%d/%Y')

# Create occupation display
eo_jobs['occupation'] = eo_jobs['occupation_series'].astype(str) + ' - ' + eo_jobs['occupation_name'].fillna('Unknown')

# Select columns to display
display_cols = ['position_title', 'occupation', 'hiring_agency', 'position_location', 
                'grade_code', 'service_type', 'open_date', 'close_date']
detail_table = eo_jobs[display_cols].copy()
detail_table.columns = ['Position Title', 'Occupation', 'Agency', 'Location', 
                       'Grade', 'Service', 'Open', 'Close']

print(f"All {len(detail_table)} job postings with the 'advancing President's Executive Orders' question:")

# Display table
gt_detail = (
    GT(detail_table)
    .tab_header(title=f"All {len(detail_table)} Job Postings with Executive Order Question")
    .tab_options(row_striping_include_table_body=False)
    .opt_row_striping(row_striping=False)
    .sub_missing(missing_text="N/A")
)
gt_detail.show()
```

## Summary

```{python}
#| output: asis
summary_text = f"""
This analysis examined **{total_scraped:,}** scraped federal job questionnaires and found that **{total_with_eo:,}** ({percentage_with_eo:.1f}%) contain the specific question: "How would you help advance the President's Executive Orders and policy priorities in this role?"

The agencies and positions that include this question in their assessment are shown in the tables above.
"""
print(summary_text)
```

## Failed Questionnaire Scrapes

```{python}
# Check if there are any scraping failures by looking for error files or logs
# Since we only analyze successfully scraped files, we'll report on coverage

total_links = len(links_df)
total_scraped = len(scraped_df)
total_not_scraped = total_links - total_scraped

print(f"Scraping Coverage:")
print(f"  Total questionnaire links found: {total_links:,}")
print(f"  Successfully scraped and analyzed: {total_scraped:,}")
print(f"  Not scraped or failed: {total_not_scraped:,}")
print(f"  Coverage rate: {(total_scraped/total_links*100):.1f}%")

# If we want to see which ones weren't scraped (optional - can be large)
if total_not_scraped > 0 and total_not_scraped < 50:  # Only show if reasonable number
    # Find questionnaires that weren't scraped
    not_scraped_df = links_df[~links_df['questionnaire_id'].isin(scraped_ids)].copy()
    
    if len(not_scraped_df) > 0:
        print(f"\nShowing {len(not_scraped_df)} unscraped questionnaires:")
        
        # Create display table
        display_cols = ['position_title', 'hiring_agency', 'position_location', 'grade_code']
        if all(col in not_scraped_df.columns for col in display_cols):
            failed_table = not_scraped_df[display_cols].head(20).copy()  # Show max 20
            failed_table.columns = ['Position Title', 'Agency', 'Location', 'Grade']
            
            # Create GT table without heatmap
            gt_failed = (
                GT(failed_table)
                .tab_header(title=f"Sample of Unscraped Questionnaires (showing up to 20)")
                .tab_options(row_striping_include_table_body=False)
        .opt_row_striping(row_striping=False)
                .sub_missing(missing_text="N/A")
            )
            gt_failed.show()
```

